{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoq2_cCZ0im5",
        "outputId": "640a9ef7-3916-4a96-a916-919b85335d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images\n",
            "License(s): CC0-1.0\n",
            "Downloading melanoma-skin-cancer-dataset-of-10000-images.zip to /content\n",
            "  0% 0.00/98.7M [00:00<?, ?B/s]\n",
            "100% 98.7M/98.7M [00:00<00:00, 1.05GB/s]\n"
          ]
        }
      ],
      "source": [
        "# 1. Kaggle API 설치 및 설정\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# 본인의 kaggle.json 파일을 /content/kaggle.json 경로에 업로드 후 실행하세요.\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 2. 데이터셋 다운로드 및 압축 해제\n",
        "!kaggle datasets download -d hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images\n",
        "!unzip -q melanoma-skin-cancer-dataset-of-10000-images.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 장치 설정 (GPU 우선 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 데이터셋 경로\n",
        "DATASET_PATH = 'melanoma_cancer_dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuxEvf5J0tGB",
        "outputId": "14cba545-b64b-43eb-d2b1-52c6f4092a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 변환 정의\n",
        "# Keras의 preprocess_input과 resize를 대체합니다.\n",
        "# RegNet 모델이 ImageNet으로 사전 훈련될 때 사용된 정규화 값입니다.\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((320, 320)),\n",
        "        transforms.RandomHorizontalFlip(), # 데이터 증강\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((320, 320)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "class MelanomaDataset(Dataset):\n",
        "    def __init__(self, root_dir, mode='train', transform=None):\n",
        "        self.meta = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # Keras 코드의 os.walk 부분을 재구성하여 파일 경로와 레이블 리스트를 생성합니다.\n",
        "        data_path = os.path.join(root_dir, 'train' if mode == 'train' else 'test')\n",
        "        for label, category in enumerate(['benign', 'malignant']):\n",
        "            class_path = os.path.join(data_path, category)\n",
        "            for filename in os.listdir(class_path):\n",
        "                if filename.endswith('.jpg'):\n",
        "                    path = os.path.join(class_path, filename)\n",
        "                    self.meta.append((path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.meta[idx]\n",
        "        # Keras의 load_image 함수 부분을 대체합니다.\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# 데이터셋 인스턴스 생성\n",
        "train_dataset = MelanomaDataset(root_dir=DATASET_PATH, mode='train', transform=data_transforms['train'])\n",
        "valid_dataset = MelanomaDataset(root_dir=DATASET_PATH, mode='val', transform=data_transforms['val'])\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# 데이터셋 크기 확인\n",
        "print(f'Train dataset size: {len(train_dataset)}')\n",
        "print(f'Validation dataset size: {len(valid_dataset)}')\n",
        "\n",
        "# 데이터로더 테스트\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f'Image batch shape: {images.shape}')\n",
        "print(f'Label batch shape: {labels.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWWWchWd1XJk",
        "outputId": "1719e5f6-a3ad-46df-fa27-08f36bc867c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 9605\n",
            "Validation dataset size: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([32, 3, 320, 320])\n",
            "Label batch shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras의 base_model에 해당합니다. ImageNet으로 사전 훈련된 가중치를 사용합니다.\n",
        "model = models.regnet_x_3_2gf(pretrained=True)\n",
        "\n",
        "# Keras의 GlobalAvgPool2D + Dense(1, activation='sigmoid') 부분을 대체합니다.\n",
        "# RegNet의 마지막 fully-connected layer를 이진 분류에 맞게 교체합니다.\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1) # 출력 1개\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# 모델 구조 확인 (Keras의 summary와 유사)\n",
        "# print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDCzaYAD1p5N",
        "outputId": "510f88b0-c749-4883-ecdd-de565644304e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RegNet_X_3_2GF_Weights.IMAGENET1K_V1`. You can also use `weights=RegNet_X_3_2GF_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/regnet_x_3_2gf-f342aeae.pth\" to /root/.cache/torch/hub/checkpoints/regnet_x_3_2gf-f342aeae.pth\n",
            "100%|██████████| 58.8M/58.8M [00:01<00:00, 46.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # --- 훈련 단계 ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_acc = correct_predictions / total_samples\n",
        "\n",
        "        # --- 검증 단계 ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).view(-1, 1)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                preds = torch.sigmoid(outputs) > 0.5\n",
        "                val_corrects += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_epoch_loss = val_loss / val_total\n",
        "        val_epoch_acc = val_corrects / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "              f\"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f} - \"\n",
        "              f\"val_loss: {val_epoch_loss:.4f} - val_accuracy: {val_epoch_acc:.4f}\")\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# --- 1단계: 전이 학습 (Feature Extraction) ---\n",
        "# Keras의 `base_model.trainable = False`에 해당\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# 새로 추가한 fc 레이어의 파라미터만 학습하도록 설정\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# fc 레이어의 파라미터만 optimizer에 전달\n",
        "optimizer_transfer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "print(\"--- Start Transfer Learning (Training Head) ---\")\n",
        "# 5 에포크 훈련\n",
        "train_model(model, criterion, optimizer_transfer, train_loader, valid_loader, epochs=5)\n",
        "\n",
        "\n",
        "# --- 2단계: 미세 조정 (Fine-tuning) ---\n",
        "# Keras의 `base_model.trainable = True`에 해당 (문서에서는 모든 가중치의 프리즈를 푼다고 언급)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 전체 모델의 파라미터를 optimizer에 전달. 보통 더 낮은 학습률(learning rate)을 사용합니다.\n",
        "optimizer_finetune = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "print(\"\\n--- Start Fine-tuning (Training All Layers) ---\")\n",
        "# 15 에포크 추가 훈련\n",
        "train_model(model, criterion, optimizer_finetune, train_loader, valid_loader, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA0JWU7-1vtA",
        "outputId": "70793c36-7652-412a-e364-ab1684b82d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start Transfer Learning (Training Head) ---\n",
            "Epoch 1/5 - loss: 0.3318 - accuracy: 0.8612 - val_loss: 0.2356 - val_accuracy: 0.9070\n",
            "Epoch 2/5 - loss: 0.2540 - accuracy: 0.8970 - val_loss: 0.2299 - val_accuracy: 0.9090\n",
            "Epoch 3/5 - loss: 0.2436 - accuracy: 0.9017 - val_loss: 0.2268 - val_accuracy: 0.9080\n",
            "Epoch 4/5 - loss: 0.2365 - accuracy: 0.9045 - val_loss: 0.2430 - val_accuracy: 0.9060\n",
            "Epoch 5/5 - loss: 0.2230 - accuracy: 0.9107 - val_loss: 0.2599 - val_accuracy: 0.8990\n",
            "\n",
            "--- Start Fine-tuning (Training All Layers) ---\n",
            "Epoch 1/15 - loss: 0.1991 - accuracy: 0.9208 - val_loss: 0.2005 - val_accuracy: 0.9150\n",
            "Epoch 2/15 - loss: 0.1246 - accuracy: 0.9503 - val_loss: 0.1975 - val_accuracy: 0.9230\n",
            "Epoch 3/15 - loss: 0.0887 - accuracy: 0.9678 - val_loss: 0.2165 - val_accuracy: 0.9210\n",
            "Epoch 4/15 - loss: 0.0629 - accuracy: 0.9771 - val_loss: 0.2543 - val_accuracy: 0.9240\n",
            "Epoch 5/15 - loss: 0.0483 - accuracy: 0.9826 - val_loss: 0.2703 - val_accuracy: 0.9320\n",
            "Epoch 6/15 - loss: 0.0426 - accuracy: 0.9848 - val_loss: 0.4182 - val_accuracy: 0.8910\n",
            "Epoch 7/15 - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.2723 - val_accuracy: 0.9260\n",
            "Epoch 8/15 - loss: 0.0281 - accuracy: 0.9899 - val_loss: 0.2818 - val_accuracy: 0.9210\n",
            "Epoch 9/15 - loss: 0.0409 - accuracy: 0.9863 - val_loss: 0.2692 - val_accuracy: 0.9250\n",
            "Epoch 10/15 - loss: 0.0378 - accuracy: 0.9857 - val_loss: 0.3041 - val_accuracy: 0.9130\n",
            "Epoch 11/15 - loss: 0.0283 - accuracy: 0.9897 - val_loss: 0.3178 - val_accuracy: 0.9270\n",
            "Epoch 12/15 - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.3518 - val_accuracy: 0.9300\n",
            "Epoch 13/15 - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.3417 - val_accuracy: 0.9340\n",
            "Epoch 14/15 - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.4235 - val_accuracy: 0.9230\n",
            "Epoch 15/15 - loss: 0.0184 - accuracy: 0.9933 - val_loss: 0.3907 - val_accuracy: 0.9300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gG-y7HH2a2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}